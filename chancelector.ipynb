{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, json, datetime\n",
    "import os, sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin\n",
    "import uuid\n",
    "from hashlib import md5\n",
    "import dateparser\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "first_time=False\n",
    "\n",
    "\n",
    "def recombine_link_list(link_list):\n",
    "    rlist = []\n",
    "    t = \"\"\n",
    "    s=-1\n",
    "    for (url, text ,subtitle, desc, user, date ) in link_list:\n",
    "        s = s + 1\n",
    "        try:\n",
    "            rlist.append((s, url, text, subtitle, desc, user, dateparser.parse(date).timestamp()))\n",
    "        except:\n",
    "            print ((s, url, text, subtitle, desc, user, dateparser.parse(date)))\n",
    "            rlist.append((s, url, text, subtitle, desc, user, dateparser.parse(date)))\n",
    "    return rlist\n",
    "\n",
    "def recombine_anno_list(anno_list):\n",
    "    rlist = []\n",
    "    t = \"\"\n",
    "    s=-1\n",
    "    for (a,u,d) in anno_list:\n",
    "        if u==\"\" and d==\"\":\n",
    "            t=t+\" \"+a\n",
    "        else:\n",
    "            s=s+1\n",
    "            t=(t+\" \"+a)\n",
    "            #rlist.append((s,t.replace(\"\\r\\n\", \"\"),u,d))\n",
    "            rlist.append((s,t,u,dateparser.parse(d).timestamp()))\n",
    "            t=\"\"\n",
    "    return rlist\n",
    "\n",
    "def scrape_link_values(link_list_soup_element):\n",
    "    link_url = link_list_soup_element.find('a')['href']\n",
    "    try:\n",
    "        link_text = \"\".join(link_list_soup_element.find_next_sibling().find('nobr').strings)\n",
    "    except:\n",
    "        link_text = \"\"\n",
    "    link_subtitle = \"\".join(link_list_soup_element.find('a').strings)\n",
    "    link_desc = \"\".join(\"\".join(link_list_soup_element.find_next_sibling().find('br').next_element))\n",
    "    link_user = \"\".join(link_list_soup_element.find_next_sibling().find('a').strings)\n",
    "    lstring = \"\".join(link_list_soup_element.find_next_sibling().strings)\n",
    "    link_date = \"\".join(link_list_soup_element.find_next_sibling().strings)[lstring.find(link_user)+\n",
    "            len(link_user)+2:lstring.find(']',lstring.find(link_user)+len(link_user))]\n",
    "    try:\n",
    "        link_date = datetime.strptime(\n",
    "            re.search(\"([Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec]{3} \\d{2} \\d{4})\", link_date).group(1), \"%b %d %Y\").isoformat()\n",
    "    except:\n",
    "        pass\n",
    "        #print(link_date)\n",
    "    return link_url, link_text, link_subtitle, link_desc, link_user, link_date\n",
    "\n",
    "def scrape_annotations(anno_element):\n",
    "    #print(anno_element)\n",
    "    anno_content = \"\".join(anno_element.find('font', attrs={'class':'fcs'}).strings)\n",
    "    try:\n",
    "        anno_user = \"\".join(anno_element.find('td', attrs={'class':'fcs'}).find('a').strings)\n",
    "        #anno_date = datetime.datetime.now()\n",
    "        anno_date = \"\".join(anno_element.find('td', attrs={'class':'fcs'}).strings)[-11:]\n",
    "    except:\n",
    "        anno_user = \"\"                        \n",
    "        anno_date = \"\"\n",
    "    return anno_content, anno_user, anno_date\n",
    "\n",
    "\n",
    "\n",
    "def get_links(s, url):\n",
    "    r = s.get (url)\n",
    "    page_links_regex = re.compile(\"<a class=\\\"(?:newidea|oldidea)\\\" href=\\\"(/idea/.*?)\\\"\")\n",
    "    link_harvest = [urljoin(url,l).split(\"#\")[0] for l in page_links_regex.findall(r.text)]\n",
    "    return link_harvest\n",
    "\n",
    "def idea_components(hb_link):\n",
    "    l=hb_link\n",
    "    r = s.get(l)\n",
    "    fetch_time=datetime.datetime.now().timestamp()\n",
    "    soup=bs(r.text,\"html\")\n",
    "    mainpanel = soup.find('td', attrs={'class':'mainpanel'})\n",
    "    idea_header = mainpanel.findAll('table')[2]\n",
    "    title = str(\"\".join(idea_header.find('a', attrs={'name':'idea'}).strings))\n",
    "    fetch_id = str(uuid.uuid4())\n",
    "    description = \"\".join(mainpanel.find('font', attrs={'class':'fcl'}).strings)\n",
    "    #votes = self.getvotes(\"\".join(mainpanel.find('td', attrs={'class':'controls'}).find('td', attrs={'valign':'top', 'align':'center'}).strings).replace(\"(\",\"\").replace(\")\",\"\").split(\",\"))\n",
    "    copy = str(\"\".join(idea_header.find('div', attrs={'class':'copy'}).strings))\n",
    "    (user, text_date) = ( n.strip() for n in str(\"\".join(idea_header.find('td', attrs={'class':'fcm'}).strings)).split(\",\"))\n",
    "    #idate = datetime.datetime.strptime(text_date, \"%b %d %Y\").isoformat()\n",
    "    idate=dateparser.parse(text_date).timestamp()\n",
    "    links = recombine_link_list([scrape_link_values(n) for n in idea_header.findAll('font', attrs={'class':'fcm'})])\n",
    "    annos = recombine_anno_list([scrape_annotations(n) for n in idea_header.next_siblings if n.name=='table'])\n",
    "    #print(\"\".join([str(j) for j in [title, description, copy, user, idate, links, annos]]).encode(\"utf-8\"))\n",
    "    ihash = md5(\"\".join([str(j) for j in [title, description, copy, user, idate, links, annos]]).encode(\"utf-8\")).hexdigest()\n",
    "    return {\n",
    "                 \"fetch_id\" : fetch_id,\n",
    "                 \"url\" : l, \n",
    "                 \"hash\" : ihash,\n",
    "                 \"title\":title, \n",
    "                 \"description\" : description, \n",
    "                 \"copy\" : copy, \n",
    "                 \"user\" : user, \n",
    "                 \"idea_date\" : idate, \n",
    "                 \"links\": links, \n",
    "                 \"annos\" : annos,\n",
    "                 \"fetch_date\" : fetch_time\n",
    "            }\n",
    "\n",
    "\n",
    "# SQLite requires dates be converted according to some convention - here we'll use integer seconds since epoch\n",
    "# or whatever is convenient.\n",
    "# Also, we have a multi-table structure, since annos and links are collections of records themselves.\n",
    "# So the structure looks like:\n",
    "\n",
    "#   +--------------------+\n",
    "#   |  idea_fetch        |\n",
    "#   +--------------------+\n",
    "#   |  fetch_id (pk)     |\n",
    "#   |  url               |\n",
    "#   |  hash              |\n",
    "#   |  title             |\n",
    "#   |  description       |\n",
    "#   |  copy              |\n",
    "#   |  user              |\n",
    "#   |  idea_date         |\n",
    "#   |  fetch_date        |\n",
    "#   +--------------------+\n",
    "\n",
    "\n",
    "def sql_create_schema(conn,first_time=False):\n",
    "    if first_time:\n",
    "        c = conn.cursor()\n",
    "        c.execute( \"\"\"DROP TABLE idea_fetch\"\"\")\n",
    "        c.close()\n",
    "        c = conn.cursor()\n",
    "        c.execute( \"\"\"CREATE TABLE idea_fetch\n",
    "                    (   fetch_id text,\n",
    "                        url text, \n",
    "                        hash text, \n",
    "                        title text, \n",
    "                        description text, \n",
    "                        copy text, \n",
    "                        user text, \n",
    "                        idea_date integer, \n",
    "                        fetch_date integer)\"\"\")\n",
    "        c.close()\n",
    "        c = conn.cursor()\n",
    "        c.execute( \"\"\"DROP TABLE anno_fetch\"\"\")\n",
    "        c.close()\n",
    "        c = conn.cursor()\n",
    "        c.execute( \"\"\"CREATE TABLE anno_fetch\n",
    "                    (   fetch_id text,\n",
    "                        anno_seq integer, \n",
    "                        anno_text text, \n",
    "                        anno_user text, \n",
    "                        anno_date integer\n",
    "                        )\"\"\")\n",
    "        c.close()\n",
    "        c = conn.cursor()\n",
    "        c.execute( \"\"\"DROP TABLE link_fetch\"\"\")\n",
    "        c.close()\n",
    "        c = conn.cursor()\n",
    "        c.execute( \"\"\"CREATE TABLE link_fetch\n",
    "                    (   fetch_id text,\n",
    "                        link_seq integer, \n",
    "                        link_url text, \n",
    "                        link_rickroll text, \n",
    "                        link_text text, \n",
    "                        link_anno text,\n",
    "                        link_user text, \n",
    "                        link_date integer\n",
    "                        )\"\"\")\n",
    "        c.close()\n",
    "\n",
    "        return True\n",
    "    \n",
    "\n",
    "def store_fetch_record(c,record):\n",
    "    idea_insert_sql = \"\"\"INSERT INTO idea_fetch VALUES\n",
    "                        ( ?, ?, ?, ?, ?, ?, ?, ?, ? )\"\"\"\n",
    "    anno_insert_sql = \"\"\"INSERT INTO anno_fetch VALUES\n",
    "                        ( ?, ?, ?, ?, ? )\"\"\"\n",
    "    link_insert_sql = \"\"\"INSERT INTO link_fetch VALUES\n",
    "                    ( ?, ?, ?, ?, ?, ?, ?, ? )\"\"\"\n",
    "    links_pk = uuid.uuid4()\n",
    "    annos_pk = uuid.uuid4()\n",
    "    idea_values = [record[\"fetch_id\"], \n",
    "                   record[\"url\"], \n",
    "                   record[\"hash\"], \n",
    "                   record[\"title\"], \n",
    "                   record[\"description\"], \n",
    "                   record[\"copy\"], \n",
    "                   record[\"user\"], \n",
    "                   record[\"idea_date\"], \n",
    "                   record[\"fetch_date\"]]\n",
    "    #for e,v in enumerate(idea_values):\n",
    "    #    print(e,v)\n",
    "    c.execute(idea_insert_sql, idea_values)\n",
    "    for anno in record['annos']:\n",
    "        anno_values = [record[\"fetch_id\"], \n",
    "                       anno[0], \n",
    "                       anno[1], \n",
    "                       anno[2], \n",
    "                       anno[3]]\n",
    "        c.execute(anno_insert_sql, anno_values)\n",
    "    \n",
    "    for link in record['links']:\n",
    "        link_values = [record[\"fetch_id\"], \n",
    "                       link[0], \n",
    "                       link[1], \n",
    "                       link[2], \n",
    "                       link[3],\n",
    "                       link[4],\n",
    "                       link[5],\n",
    "                       link[6]]\n",
    "        c.execute(link_insert_sql, link_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print (first_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If an idea component is \"novel\" i.e. it has a hash that's not on file, then it can be saved for posterity\n",
    "# It also qualifies for a review for any content that matches the search criteria. \n",
    "# The details of successful searches are then logged independently in such a way that they can be used to \n",
    "# filter out repeat matches. \n",
    "\n",
    "# What should be the logging mechanism? SQLlite probably. Makes sense to create a database to host and persist\n",
    "# the content. \n",
    "conn=None\n",
    "c=None\n",
    "conn = sqlite3.connect('hb_records.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 'https://www.halfbakery.com/idea/_22Duck_22_20and_20_22Ducking_22_20Word_20Processor_20Verification', '_22Duck_22_20and_20...ssor_20Verification', 'Former Nazi Chancellor of Germany mention #1', 'Last line of the idea post by doctorremulac3 on 23-NOV-2020 [', 'doctorremulac3', None)\n",
      "(0, 'http://www.halfbakery.com/idea/Telepresence_20Airship', 'http://www.halfbake...epresence_20Airship', 'Telepresence Airship', 'Thank you [spacemoggy] for the inspiration. [', 'Worldgineer', None)\n",
      "(1, 'https://www.halfbakery.com/view/ftm=r86400:fs=hitler:s=Q:d=iq:dn=100:ds=1', 'https://www.halfbak...=Q:d=iq:dn=100:ds=1', 'Start here...', 'Run this at same time each day and parse/compare results last seen / previous search hit? [', 'kdf', None)\n",
      "['https://www.halfbakery.com/idea/Plaid_20conductor_20(Redundant_20Array_20of_20Independent_20Conductors)', 'https://www.halfbakery.com/idea/Days_20Since_20Hitler_20Was_20Mentioned_20Here', 'https://www.halfbakery.com/idea/Milligram', 'https://www.halfbakery.com/idea/Hitler-Claus', 'https://www.halfbakery.com/idea/Gourmet_20dog', 'https://www.halfbakery.com/idea/Cell_20phone_20tourism', 'https://www.halfbakery.com/idea/light', 'https://www.halfbakery.com/idea/Non-National_20Company_20(NNC)', 'https://www.halfbakery.com/idea/Dinnerware_20tearoffs', 'https://www.halfbakery.com/idea/Moisture_20wiicking_2c_20or_20maybe_20extra_20dry_20garments_3a_20Diapers', 'https://www.halfbakery.com/idea/MentorBubbles_3b_20view_20the_20internet_20versions_20of_20the_20people_20who_20are_20doing_20well', 'https://www.halfbakery.com/idea/30-300_25_20better_20foam_20earplugs', 'https://www.halfbakery.com/idea/Completely_20Realistic_20Fake_20Candle', 'https://www.halfbakery.com/idea/Hitler', 'https://www.halfbakery.com/idea/Eugenics_20SEO', 'https://www.halfbakery.com/idea/Holographic_20sparkle_20concrete', 'https://www.halfbakery.com/idea/F_fcrst_20annual_20HalfBakery_20_93Wo_20ist_20der_20F_fchrer_20_3f_94_20programming_20competition_2e']\n"
     ]
    }
   ],
   "source": [
    "# This makes use of a search url, returning all the ideas posted today\n",
    "url = \"https://www.halfbakery.com/view/ftm=r86400:s=Qr:d=irq:dn={m}:ds=0:n=Today_27s_20Notions:i=A_20list_20of_20todays_20ideas_20and_20annotations:t=Today_27s_20Notions\".format(m=100)\n",
    "s = requests.Session()\n",
    "contents = []\n",
    "link_harvest = get_links(s, url)\n",
    "for l in link_harvest:\n",
    "    contents.append(idea_components(l))\n",
    "lindex = [c['url'] for c in contents]    \n",
    "#conn.row_factory = sqlite3.Row\n",
    "# If this is the first time running, then we need to create the schema\n",
    "if first_time is True:\n",
    "    first_time = False\n",
    "    sql_create_schema(conn, first_time)\n",
    "print (lindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n",
      "Hashmatch - no update\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = conn.cursor()\n",
    "retrieve_latest_sql = \"\"\"\n",
    "                select i.url, i.hash, i.fetch_date \n",
    "                from (\n",
    "                    select url, max(fetch_date) max_fetch_date\n",
    "                    from (\n",
    "                        select url, fetch_date\n",
    "                         from idea_fetch\n",
    "                         where url in ({in_list}))\n",
    "                    group by url) as latest_v\n",
    "                    join idea_fetch i on \n",
    "                    i.url = latest_v.url and\n",
    "                    i.fetch_date = latest_v.max_fetch_date \n",
    "                \"\"\".format(in_list = \",\".join([\"?\" for l in lindex]))\n",
    "\n",
    "rs = c.execute(retrieve_latest_sql, lindex)\n",
    "r_cols = rs.description\n",
    "content_filter=[]\n",
    "for r in rs:\n",
    "    lindex_i = lindex.index(r[0])\n",
    "    if contents[lindex_i]['hash']==r[1]:\n",
    "        print (\"Hashmatch - no update\")\n",
    "        content_filter.append(r[0])\n",
    "    else:\n",
    "        print (\"Hashfail - got update\")\n",
    "        \n",
    "rs.close()\n",
    "c.close()\n",
    "\n",
    "save_list = list(set(lindex).difference(set(content_filter)))\n",
    "\n",
    "for c in contents:\n",
    "    if c['url'] in save_list:\n",
    "        store_fetch_record(conn, c)\n",
    "        print(\"Saving\", c['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.halfbakery.com/idea/30-300_25_20better_20foam_20earplugs', 'ebf93ada436691c17f2c903eea261df2', 1608226307.98454]\n",
      "['https://www.halfbakery.com/idea/Cell_20phone_20tourism', 'a6f8a9eb2ba940e270a623217f6a13af', 1608226305.998213]\n",
      "['https://www.halfbakery.com/idea/Completely_20Realistic_20Fake_20Candle', '526ae5b7cd8eaff0e38bd3d920f423c2', 1608226308.268491]\n",
      "['https://www.halfbakery.com/idea/Days_20Since_20Hitler_20Was_20Mentioned_20Here', 'e983ab3e8492c98e23f6ac875f6175d7', 1608226303.061566]\n",
      "['https://www.halfbakery.com/idea/Dinnerware_20tearoffs', 'e4702c677d8e609ac21d5402361fc230', 1608226307.093517]\n",
      "['https://www.halfbakery.com/idea/Eugenics_20SEO', 'fdc2033e15427364dfa95f461bd122cf', 1608226308.881248]\n",
      "['https://www.halfbakery.com/idea/F_fcrst_20annual_20HalfBakery_20_93Wo_20ist_20der_20F_fchrer_20_3f_94_20programming_20competition_2e', 'b92c80a6b32afe35f5e8d46edd0238c5', 1608226309.480555]\n",
      "['https://www.halfbakery.com/idea/Gourmet_20dog', 'd91480008ee76740cef1058dec950b7c', 1608226305.699364]\n",
      "['https://www.halfbakery.com/idea/Hitler', '1e1b0590c031dc9a13f584a1b8bf48e7', 1608226308.58233]\n",
      "['https://www.halfbakery.com/idea/Hitler-Claus', 'b89d54e851d48cc2b50b48083da18838', 1608226305.43479]\n",
      "['https://www.halfbakery.com/idea/Holographic_20sparkle_20concrete', 'f0c02a940e1596f9b5195914c2056c36', 1608226309.191411]\n",
      "['https://www.halfbakery.com/idea/MentorBubbles_3b_20view_20the_20internet_20versions_20of_20the_20people_20who_20are_20doing_20well', '5c38cf351ddeb67f10696f0308cb5398', 1608226307.687863]\n",
      "['https://www.halfbakery.com/idea/Milligram', 'bf760c535f6c84423dba11c398757ed7', 1608226305.115041]\n",
      "['https://www.halfbakery.com/idea/Moisture_20wiicking_2c_20or_20maybe_20extra_20dry_20garments_3a_20Diapers', '24554049f1f07bc7752bb4b6175c915d', 1608226307.407422]\n",
      "['https://www.halfbakery.com/idea/Non-National_20Company_20(NNC)', 'a2e0f9939f4171bc6b24b878b4e3ee5a', 1608226306.826914]\n",
      "['https://www.halfbakery.com/idea/Plaid_20conductor_20(Redundant_20Array_20of_20Independent_20Conductors)', '1a8b6625e1bc5ad41623ea86e3390045', 1608226304.838661]\n",
      "['https://www.halfbakery.com/idea/Plaid_20conductor_20(Redundant_20Array_20of_20Independent_20Conductors)', 'f279c9b3b621220394b7b34a1c5aef83', 1608226452.129921]\n",
      "['https://www.halfbakery.com/idea/light', 'fb8e7dc01c6632e7da2151210b819aa1', 1608226306.545715]\n"
     ]
    }
   ],
   "source": [
    "c = conn.cursor()\n",
    "rs = c.execute(\"\"\"select url, hash, fetch_date\n",
    "                from idea_fetch \n",
    "                order by url, fetch_date \"\"\")\n",
    "r_cols = rs.description\n",
    "\n",
    "for r in rs:\n",
    "    #print ( [(r_cols[e][0], r[e]) for e,v in enumerate(r)] )\n",
    "    print ( [(r[e]) for e,v in enumerate(r)] )\n",
    "rs.close()\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1608226309.781665"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
